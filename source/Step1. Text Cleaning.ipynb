{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1. Text Cleaning\n",
    "\n",
    "This Jupyter notebook demonstrates how to clean COVID-19 article data by using python code. Data cleaning aims to make data to be useful for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = '../data/search_results/'\n",
    "JSON_FILE = 'search_results.json'\n",
    "OUTPUT_PATH = '../data/processed_data/'\n",
    "OUTPUT_FILE = 'processed_data.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(558964, 18)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read file\n",
    "json_f = json.load(open(INPUT_PATH+JSON_FILE))\n",
    "# convert JSON to dictionary\n",
    "tmp_dict= json.loads(json_f)\n",
    "# convert dictionary to dataframe\n",
    "df = pd.json_normalize(tmp_dict) \n",
    "# remove unnecessary columns from the dataframe\n",
    "selected_columns = ['_id', 'URL', 'created', 'ISSN', 'container-title', 'author', 'DOI',\n",
    "       'published', 'subject', 'title', 'link', 'source', 'type', 'publisher',\n",
    "       'volume', 'last-updated', 'issue', 'funder']\n",
    "df = df[selected_columns]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_white_spaces(text):\n",
    "    # 1) Remove newline\n",
    "    redun_lines = [\"\\n\", chr(13)]\n",
    "    for line in redun_lines:\n",
    "        text = text.replace(line, \" \")\n",
    "    # 2) Remove >1 conseq Spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    # 3) other whitespaces* (incl 1,2?)\n",
    "    text = \" \".join(re.split(r\"\\s+\", text))\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_tags(dataObj, tags_only=False):\n",
    "    \"\"\"\n",
    "    Cleaning - remove tags, URLs, special characters\n",
    "    \"\"\"\n",
    "    # Del Tag + Content (sub-titles):   <jats:title content-type=\"abstract-subheading\">Purpose</jats:title>\n",
    "    redun_tags = ['<jats:title>', '<title>']\n",
    "    for tag in redun_tags:\n",
    "        start = dataObj.find(tag[:-1])\n",
    "        while start != -1:\n",
    "            end = dataObj.find(\"</\" + tag[1:-2], start)  # length 13      (excl last 2: for </tag   >\n",
    "            if end != -1: dataObj = dataObj.replace(dataObj[start:end + 13], \" \")\n",
    "            start = dataObj.find(tag[:-1], start + 5)  # NEXT start (SKIP current - *if prev without end)\n",
    "\n",
    "    # Del ALL Tags <....>    # redun_tags = [\"<p>\", \"<jats:p>\", \"<jats:sec>\", \"<sec>\", \"<jats:italic>\", \"<jats:bold>\", \"<jats:p id=\"\"p1\"\">\"]\n",
    "    dataObj = re.sub('<[^<]+?>', ' ', dataObj)\n",
    "\n",
    "    # Del URLs\n",
    "    re_url = 'https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|info)/' + '[a-z0-9.\\-]'\n",
    "    dataObj = re.sub(re_url, '', dataObj)\n",
    "\n",
    "    if not tags_only:\n",
    "        ### Del Symbols\n",
    "        dataObj = re.sub('[&;]\\d+;*', ' ', dataObj)  # [0-9] -> \\d, [;] -> ;\n",
    "        dataObj = re.sub('&[A-Z]{4}', ' ', dataObj)\n",
    "        dataObj = re.sub('&\\W{2,10};', ' ', dataObj)  # [\\W] -> \\W\n",
    "        dataObj = re.sub('&#\\d{2,4};', ' ', dataObj)\n",
    "        redun = [\"amp\", \";lt\", \";gt\", \"&lt\", \"&gt\", \";p\", \"div\", \"&#x0D;\", \"ldquo\", \"rdquo\", \" \", \" \", \" \", \"#160\", \"/p\", \";\"]\n",
    "        for substr in redun:\n",
    "            dataObj = dataObj.replace(substr, \" \")\n",
    "\n",
    "    return remove_white_spaces(dataObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleaning_text(text):\n",
    "    \"\"\"\n",
    "        Remove stop-words\n",
    "        No digits\n",
    "        No word length less than 3 \n",
    "        Convert to lowercase\n",
    "    \"\"\"\n",
    "\n",
    "    cleantext = clear_tags(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    pos_family = {\n",
    "        'noun': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "        'pron': ['PRP', 'PRP$', 'WP', 'WP$'],\n",
    "        'verb': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
    "        'adj': ['JJ', 'JJR', 'JJS'],\n",
    "        'adv': ['RB', 'RBR', 'RBS', 'WRB']\n",
    "    }\n",
    "    \n",
    "    cleantext = cleantext.replace('-', '_')\n",
    "    \n",
    "    stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
    "    avoiding_words = ['covid_19', 'COVID_19', 'covid', 'COVID', 'covid_', 'COVID_', \n",
    "                      'coronavirus', 'CORONAVIRUS', 'coronaviruses', 'CORONAVIRUSES',\n",
    "                      'SARS_COV_2','sars_cov_2', 'conclusion', 'CONCLUSION', \n",
    "                      'objective', 'OBJECTIVE', 'abstract', 'ABSTRACT', \n",
    "                      'background', 'BACKGROUND', 'author', 'AUTHOR', \n",
    "                      'disclosure', 'DISCLOSURE', 'title', 'TITLE']\n",
    "    \n",
    "    stopwords.extend(avoiding_words)\n",
    "    \n",
    "    regex = r\"\\b[^\\d\\W]+\\b\"\n",
    "    tokens = []\n",
    "    sentences = nltk.sent_tokenize(cleantext)\n",
    "    for s in sentences:\n",
    "        words = re.findall(regex, s)\n",
    "        pairs = nltk.pos_tag(words)\n",
    "        for pair in pairs:\n",
    "            w = list(pair)[0] \n",
    "            tag = list(pair)[1]\n",
    "        \n",
    "            if w.isupper() != True:w = w.lower() \n",
    "                \n",
    "            if tag in pos_family['noun']:\n",
    "                w = lemmatizer.lemmatize(w, 'n')\n",
    "            elif tag in pos_family['pron']: # e.g their, self, what\n",
    "                w = lemmatizer.lemmatize(w)\n",
    "            elif tag in pos_family['verb']: # e.g experienced, based, evaluating, trying, healthcare\n",
    "                w = lemmatizer.lemmatize(w, 'v')\n",
    "            elif tag in pos_family['adj']: # e.g significant, pandemic, clinical, sensitive\n",
    "                w = lemmatizer.lemmatize(w, 'a')\n",
    "            elif tag in pos_family['adv']: #e.g. sore, seriously, alone, nationally\n",
    "                w = lemmatizer.lemmatize(w, 'r')\n",
    "            \n",
    "            if w in stopwords: continue       \n",
    "            if w.isdigit(): continue\n",
    "            if len(w) <= 3: continue\n",
    "\n",
    "            tokens.append(w)\n",
    "    cleaned_text = ' '.join(tokens)\n",
    " \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['title'].apply(lambda x: cleaning_text(x))\n",
    "\n",
    "# drop zero-word articles\n",
    "df.reset_index()\n",
    "drop_index = []\n",
    "for index, row in df.iterrows():\n",
    "    if len(row['text']) < 3:\n",
    "        drop_index.append(index)\n",
    "cleaned_df = df.drop(drop_index, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result\n",
    "cleaned_df.to_csv(OUTPUT_PATH + OUTPUT_FILE, index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "public_env",
   "language": "python",
   "name": "public_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
