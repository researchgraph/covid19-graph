{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Documents Data Preprocessing\n",
    "\n",
    "This Jupyter notebook demonstrates how to preprocess COVID-19 article data by using python code. Data preprocessing aims to make data to be useful for analysis, which contains removing duplications, non-English documents, cleaning text, and reformatting data table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = '../data/covid-json/'\n",
    "OUTPUT_PATH = '../data/preprocessed_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json1 = 'covid_19.json'\n",
    "json2 = 'covid19.json'\n",
    "json3 = 'sars_cov_2.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_json(INPUT_PATH+json1)\n",
    "df2 = pd.read_json(INPUT_PATH+json2)\n",
    "df3 = pd.read_json(INPUT_PATH+json3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns\n",
    "\n",
    "column_list = ['_id',\n",
    " 'abstract',\n",
    " 'URL',\n",
    " 'created',\n",
    " 'ISSN',\n",
    " 'container-title',\n",
    " 'author',\n",
    " 'DOI',\n",
    " 'published',\n",
    " 'subject',\n",
    " 'title',\n",
    " 'link',\n",
    " 'source',\n",
    " 'type',\n",
    " 'publisher',\n",
    " 'volume',\n",
    " 'last-updated',\n",
    " 'issue',\n",
    " 'funder',\n",
    " 'pubmed-abstract']\n",
    "\n",
    "df1 = df1[column_list]\n",
    "df2 = df2[column_list]\n",
    "df3 = df3[column_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(658943, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the dataframes\n",
    "\n",
    "frames = [df1, df2, df3]\n",
    "covid_df = pd.concat(frames)\n",
    "\n",
    "# Remove duplicated articles by '_id'\n",
    "covid_df.drop_duplicates(subset=['_id'], keep='first', inplace=True)\n",
    "covid_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclude non-English articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_detection(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "    except:\n",
    "        language = \"error\"\n",
    "    return language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df['title'] = covid_df['title'].astype(str)\n",
    "covid_df['language'] = covid_df['title'].apply(lambda x: language_detection(x))\n",
    "english_covid_df = covid_df.loc[covid_df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(558566, 21)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_covid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_covid_df.to_csv(OUTPUT_PATH + \"english_covid_articles.tsv\", sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_white_spaces(text):\n",
    "    # 1) Remove newline, etc. (into Space)\n",
    "    redun_lines = [\"\\n\", chr(13)]\n",
    "    for line in redun_lines:\n",
    "        text = text.replace(line, \" \")\n",
    "    # 2) Remove >1 conseq Spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    # 3) other whitespaces* (incl 1,2?)\n",
    "    text = \" \".join(re.split(r\"\\s+\", text))\n",
    "   \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_tags(dataObj, tags_only=False):\n",
    "    \"\"\"\n",
    "    Cleaning - remove tags, URLs, special characters\n",
    "    \"\"\"\n",
    "    # Del Tag + Content (sub-titles):   <jats:title content-type=\"abstract-subheading\">Purpose</jats:title>\n",
    "    redun_tags = ['<jats:title>', '<title>']\n",
    "    for tag in redun_tags:\n",
    "        start = dataObj.find(tag[:-1])\n",
    "        while start != -1:\n",
    "            end = dataObj.find(\"</\" + tag[1:-2], start)  # length 13      (excl last 2: for </tag   >\n",
    "            if end != -1: dataObj = dataObj.replace(dataObj[start:end + 13], \" \")\n",
    "            start = dataObj.find(tag[:-1], start + 5)  # NEXT start (SKIP current - *if prev without end)\n",
    "\n",
    "    # Del ALL Tags <....>    # redun_tags = [\"<p>\", \"<jats:p>\", \"<jats:sec>\", \"<sec>\", \"<jats:italic>\", \"<jats:bold>\", \"<jats:p id=\"\"p1\"\">\"]\n",
    "    dataObj = re.sub('<[^<]+?>', ' ', dataObj)\n",
    "\n",
    "    # Del URLs\n",
    "    re_url = 'https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|info)/' + '[a-z0-9.\\-]'\n",
    "    dataObj = re.sub(re_url, '', dataObj)\n",
    "\n",
    "    if not tags_only:\n",
    "        ### Del Symbols\n",
    "        dataObj = re.sub('[&;]\\d+;*', ' ', dataObj)  # [0-9] -> \\d, [;] -> ;\n",
    "        dataObj = re.sub('&[A-Z]{4}', ' ', dataObj)\n",
    "        dataObj = re.sub('&\\W{2,10};', ' ', dataObj)  # [\\W] -> \\W\n",
    "        dataObj = re.sub('&#\\d{2,4};', ' ', dataObj)\n",
    "        redun = [\"amp\", \";lt\", \";gt\", \"&lt\", \"&gt\", \";p\", \"div\", \"&#x0D;\", \"ldquo\", \"rdquo\", \" \", \" \", \" \", \"#160\", \"/p\", \";\"]\n",
    "        for substr in redun:\n",
    "            dataObj = dataObj.replace(substr, \" \")\n",
    "\n",
    "    return remove_white_spaces(dataObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleaning_text(text):\n",
    "    \"\"\"\n",
    "        Remove stop-words\n",
    "        No digits\n",
    "        No word length less than 3 \n",
    "        Convert to lowercase\n",
    "    \"\"\"\n",
    "\n",
    "    cleantext = clear_tags(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    pos_family = {\n",
    "        'noun': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "        'pron': ['PRP', 'PRP$', 'WP', 'WP$'],\n",
    "        'verb': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
    "        'adj': ['JJ', 'JJR', 'JJS'],\n",
    "        'adv': ['RB', 'RBR', 'RBS', 'WRB']\n",
    "    }\n",
    "    \n",
    "    cleantext = cleantext.replace('-', '_')\n",
    "    \n",
    "    stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
    "    avoiding_words = ['covid_19', 'COVID_19', 'covid', 'COVID', 'covid_', 'COVID_', \n",
    "                      'coronavirus', 'CORONAVIRUS', 'coronaviruses', 'CORONAVIRUSES',\n",
    "                      'SARS_COV_2','sars_cov_2', 'conclusion', 'CONCLUSION', \n",
    "                      'objective', 'OBJECTIVE', 'abstract', 'ABSTRACT', \n",
    "                      'background', 'BACKGROUND', 'author', 'AUTHOR', \n",
    "                      'disclosure', 'DISCLOSURE', 'title', 'TITLE']\n",
    "    \n",
    "    stopwords.extend(avoiding_words)\n",
    "    \n",
    "    regex = r\"\\b[^\\d\\W]+\\b\"\n",
    "    tokens = []\n",
    "    sentences = nltk.sent_tokenize(cleantext)\n",
    "    for s in sentences:\n",
    "        words = re.findall(regex, s)\n",
    "        pairs = nltk.pos_tag(words)\n",
    "        for pair in pairs:\n",
    "            w = list(pair)[0] \n",
    "            tag = list(pair)[1]\n",
    "        \n",
    "            if w.isupper() != True:w = w.lower() \n",
    "                \n",
    "            if tag in pos_family['noun']:\n",
    "                w = lemmatizer.lemmatize(w, 'n')\n",
    "            elif tag in pos_family['pron']: # e.g their, self, what\n",
    "                w = lemmatizer.lemmatize(w)\n",
    "            elif tag in pos_family['verb']: # e.g experienced, based, evaluating, trying, healthcare\n",
    "                w = lemmatizer.lemmatize(w, 'v')\n",
    "            elif tag in pos_family['adj']: # e.g significant, pandemic, clinical, sensitive\n",
    "                w = lemmatizer.lemmatize(w, 'a')\n",
    "            elif tag in pos_family['adv']: #e.g. sore, seriously, alone, nationally\n",
    "                w = lemmatizer.lemmatize(w, 'r')\n",
    "            \n",
    "            if w in stopwords: continue       \n",
    "            if w.isdigit(): continue\n",
    "            if len(w) <= 3: continue\n",
    "\n",
    "            tokens.append(w)\n",
    "    cleaned_text = ' '.join(tokens)\n",
    " \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_covid_df['text'] = english_covid_df['title'].apply(lambda x: cleaning_text(x))\n",
    "english_covid_df.drop(columns=['language'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop articles after text cleaning\n",
    "\n",
    "We exclude word in the text if the word is a digit, stop-words, or special charater during the text cleaning process. Therefore, some articles remain zero-word after the text cleaning process. We drop these zero-word articles from the corpus before we apply topic modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop zero-word articles\n",
    "english_covid_df.reset_index()\n",
    "drop_index = []\n",
    "for index, row in english_covid_df.iterrows():\n",
    "    if len(row['text']) < 3:\n",
    "        drop_index.append(index)\n",
    "preprocessed_df = english_covid_df.drop(drop_index, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(557956, 19)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save text data preprocessing results\n",
    "preprocessed_df[['_id', 'text']].to_csv(OUTPUT_PATH + \"preprocessed_data.tsv\", sep='\\t', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid_venv",
   "language": "python",
   "name": "covid_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
