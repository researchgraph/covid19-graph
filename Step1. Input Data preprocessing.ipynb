{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Documents Data Preprocessing\n",
    "\n",
    "This Jupyter notebook demonstrates how to preprocess COVID-19 article data by using python code. Data preprocessing aims to make data to be useful for analysis, which contains removing duplications, non-English documents, cleaning text, and reformatting data table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = '../Data/covid-json/'\n",
    "OUTPUT_PATH = '../Data/preprocessed_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(464516, 73) (118093, 73) (98716, 71)\n"
     ]
    }
   ],
   "source": [
    "# Use pandas library to convert json to dataframe\n",
    "\n",
    "df1 = pd.read_json(INPUT_PATH+\"covid_19.json\")\n",
    "df2 = pd.read_json(INPUT_PATH+\"covid19.json\")\n",
    "df3 = pd.read_json(INPUT_PATH+\"sars_cov_2.json\")\n",
    "print(df1.shape, df2.shape, df3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547835, 73)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the dataframes\n",
    "\n",
    "frames = [df1, df2, df3]\n",
    "covid_df = pd.concat(frames)\n",
    "\n",
    "# Remove duplicated articles by '_id'\n",
    "covid_df.drop_duplicates(subset=['_id'], keep='first', inplace=True)\n",
    "covid_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclude non-English articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "    except:\n",
    "        language = 'Error'\n",
    "    return language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df['text'] = covid_df['title'].astype(str) + ' ' + covid_df['abstract'].astype(str) + ' ' + covid_df['pubmed-abstract'].astype(str)\n",
    "covid_df['language'] = covid_df['text'].apply(detect_language)\n",
    "english_covid_df = covid_df.loc[covid_df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(470574, 75)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_covid_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile(r'<[^>]+>')\n",
    "    cleantext = re.sub(cleanr, ' ', raw_html)\n",
    "    cleantext = re.sub('  ', ' ', cleantext)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "def cleaning_text(text):\n",
    "    \"\"\"\n",
    "        Remove stop-words\n",
    "        No digits\n",
    "        No word length less than 3 \n",
    "        Convert to lowercase\n",
    "    \"\"\"\n",
    "    # remove html tags\n",
    "    cleantext = cleanhtml(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    pos_family = {\n",
    "        'noun': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "        'pron': ['PRP', 'PRP$', 'WP', 'WP$'],\n",
    "        'verb': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
    "        'adj': ['JJ', 'JJR', 'JJS'],\n",
    "        'adv': ['RB', 'RBR', 'RBS', 'WRB']\n",
    "    }\n",
    "    \n",
    "    regex = r\"\\b[^\\d\\W]+\\b\"\n",
    "    tokens = []\n",
    "    cleantext = cleantext.replace('-', '_')\n",
    "    sentences = nltk.sent_tokenize(cleantext)\n",
    "    \n",
    "    for s in sentences:\n",
    "        words = re.findall(regex, s)\n",
    "        pairs = nltk.pos_tag(words)\n",
    "        for pair in pairs:\n",
    "            w = list(pair)[0] \n",
    "            tag = list(pair)[1]\n",
    "            if w in stopwords: continue\n",
    "            if w.isdigit(): continue\n",
    "            if w.isupper() != True:w = w.lower() \n",
    "            if len(w) <= 3: continue\n",
    "            if tag in pos_family['noun']:\n",
    "                w = lemmatizer.lemmatize(w, 'n')\n",
    "            elif tag in pos_family['pron']: # e.g their, self, what\n",
    "                w = lemmatizer.lemmatize(w)\n",
    "            elif tag in pos_family['verb']: # e.g experienced, based, evaluating, trying, healthcare\n",
    "                w = lemmatizer.lemmatize(w, 'v')\n",
    "            elif tag in pos_family['adj']: # e.g significant, pandemic, clinical, sensitive\n",
    "                w = lemmatizer.lemmatize(w, 'a')\n",
    "            elif tag in pos_family['adv']: #e.g. sore, seriously, alone, nationally\n",
    "                w = lemmatizer.lemmatize(w, 'r')\n",
    "            tokens.append(w)\n",
    "    cleaned_text = ' '.join(tokens)\n",
    " \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_stopwords(text):\n",
    "    \"\"\"\n",
    "        Remove common-words\n",
    "    \"\"\"\n",
    "    stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
    "    avoiding_words = ['covid_19', 'covid', 'COVID', 'covid_', 'COVID_', 'CORONAVIRUS', \n",
    "                      'SARS_COV_2', 'coronavirus', 'coronaviruses', \n",
    "                      'sars_cov_2', 'conclusion', 'CONCLUSION', 'objective', 'OBJECTIVE', 'ABSTRACT', 'BACKGROUND'\n",
    "                      'abstract', 'background', 'AUTHOR', 'DISCLOSURE', 'author', 'disclosure', 'title', 'TITLE']\n",
    "    stopwords.extend(avoiding_words)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    new_text = []\n",
    "    for word in text.split(' '):\n",
    "        if word not in stopwords:\n",
    "            new_text.append(word)\n",
    "                \n",
    "    cleaned_text = ' '.join(new_text)\n",
    " \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning part1\n",
    "\n",
    "text_cleaning = lambda x: cleaning_text(x)\n",
    "english_covid_df['cleaned_text'] = english_covid_df['text'].apply(text_cleaning)\n",
    "english_covid_df.drop(columns=['text'], inplace=True)\n",
    "english_covid_df.rename(columns={'cleaned_text':'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning part2\n",
    "\n",
    "text_cleaning = lambda x: removing_stopwords(x)\n",
    "english_covid_df['cleaned_text'] = english_covid_df['text'].apply(text_cleaning)\n",
    "english_covid_df.drop(columns=['text'], inplace=True)\n",
    "english_covid_df.rename(columns={'cleaned_text':'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop articles after text cleaning\n",
    "\n",
    "We exclude word in the text if the word is a digit, stop-words, or special charater during the text cleaning process. Therefore, some articles remain zero-word after the text cleaning process. We drop these zero-word articles from the corpus before we apply topic modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop zero-word articles\n",
    "english_covid_df.reset_index()\n",
    "drop_index = []\n",
    "for index, row in english_covid_df.iterrows():\n",
    "    if len(row['text']) < 3:\n",
    "        drop_index.append(index)\n",
    "preprocessed_df = english_covid_df.drop(drop_index, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data preprocessing results\n",
    "\n",
    "column_list = list(preprocessed_df.columns)\n",
    "column_list.remove('text')\n",
    "\n",
    "preprocessed_df[column_list].to_csv(OUTPUT_PATH + \"merged_covid_articles.tsv\", sep='\\t', encoding='utf-8', index=False)\n",
    "preprocessed_df[['_id', 'text']].to_csv(OUTPUT_PATH + \"preprocessed_data.tsv\", sep='\\t', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid_venv",
   "language": "python",
   "name": "covid_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
