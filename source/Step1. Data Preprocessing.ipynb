{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Documents Data Preprocessing\n",
    "\n",
    "This Jupyter notebook demonstrates how to preprocess COVID-19 article data by using python code. Data preprocessing aims to make data to be useful for analysis, which contains removing duplications, non-English documents, cleaning text, and reformatting data table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "import ijson\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None) is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = '../coronavirus_twenty_years_of_research/search_results/'\n",
    "OUTPUT_PATH = '../coronavirus_twenty_years_of_research/technical_validation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json1 = 'covid_19.json'\n",
    "json2 = 'covid19.json'\n",
    "json3 = 'sars_cov_2.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclude non-English articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_detection(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "    except:\n",
    "        language = \"error\"\n",
    "    return language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_white_spaces(text):\n",
    "    # 1) Remove newline, etc. (into Space)\n",
    "    redun_lines = [\"\\n\", chr(13)]\n",
    "    for line in redun_lines:\n",
    "        text = text.replace(line, \" \")\n",
    "    # 2) Remove >1 conseq Spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    # 3) other whitespaces* (incl 1,2?)\n",
    "    text = \" \".join(re.split(r\"\\s+\", text))\n",
    "   \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_tags(dataObj, tags_only=False):\n",
    "    \"\"\"\n",
    "    Cleaning - remove tags, URLs, special characters\n",
    "    \"\"\"\n",
    "    # Del Tag + Content (sub-titles):   <jats:title content-type=\"abstract-subheading\">Purpose</jats:title>\n",
    "    redun_tags = ['<jats:title>', '<title>']\n",
    "    for tag in redun_tags:\n",
    "        start = dataObj.find(tag[:-1])\n",
    "        while start != -1:\n",
    "            end = dataObj.find(\"</\" + tag[1:-2], start)  # length 13      (excl last 2: for </tag   >\n",
    "            if end != -1: dataObj = dataObj.replace(dataObj[start:end + 13], \" \")\n",
    "            start = dataObj.find(tag[:-1], start + 5)  # NEXT start (SKIP current - *if prev without end)\n",
    "\n",
    "    # Del ALL Tags <....>    # redun_tags = [\"<p>\", \"<jats:p>\", \"<jats:sec>\", \"<sec>\", \"<jats:italic>\", \"<jats:bold>\", \"<jats:p id=\"\"p1\"\">\"]\n",
    "    dataObj = re.sub('<[^<]+?>', ' ', dataObj)\n",
    "\n",
    "    # Del URLs\n",
    "    re_url = 'https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|info)/' + '[a-z0-9.\\-]'\n",
    "    dataObj = re.sub(re_url, '', dataObj)\n",
    "\n",
    "    if not tags_only:\n",
    "        ### Del Symbols\n",
    "        dataObj = re.sub('[&;]\\d+;*', ' ', dataObj)  # [0-9] -> \\d, [;] -> ;\n",
    "        dataObj = re.sub('&[A-Z]{4}', ' ', dataObj)\n",
    "        dataObj = re.sub('&\\W{2,10};', ' ', dataObj)  # [\\W] -> \\W\n",
    "        dataObj = re.sub('&#\\d{2,4};', ' ', dataObj)\n",
    "        redun = [\"amp\", \";lt\", \";gt\", \"&lt\", \"&gt\", \";p\", \"div\", \"&#x0D;\", \"ldquo\", \"rdquo\", \" \", \" \", \" \", \"#160\", \"/p\", \";\"]\n",
    "        for substr in redun:\n",
    "            dataObj = dataObj.replace(substr, \" \")\n",
    "\n",
    "    return remove_white_spaces(dataObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleaning_text(text):\n",
    "    \"\"\"\n",
    "        Remove stop-words\n",
    "        No digits\n",
    "        No word length less than 3 \n",
    "        Convert to lowercase\n",
    "    \"\"\"\n",
    "\n",
    "    cleantext = clear_tags(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    pos_family = {\n",
    "        'noun': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "        'pron': ['PRP', 'PRP$', 'WP', 'WP$'],\n",
    "        'verb': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
    "        'adj': ['JJ', 'JJR', 'JJS'],\n",
    "        'adv': ['RB', 'RBR', 'RBS', 'WRB']\n",
    "    }\n",
    "    \n",
    "    cleantext = cleantext.replace('-', '_')\n",
    "    \n",
    "    stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
    "    avoiding_words = ['covid_19', 'COVID_19', 'covid', 'COVID', 'covid_', 'COVID_', \n",
    "                      'coronavirus', 'CORONAVIRUS', 'coronaviruses', 'CORONAVIRUSES',\n",
    "                      'SARS_COV_2','sars_cov_2', 'conclusion', 'CONCLUSION', \n",
    "                      'objective', 'OBJECTIVE', 'abstract', 'ABSTRACT', \n",
    "                      'background', 'BACKGROUND', 'author', 'AUTHOR', \n",
    "                      'disclosure', 'DISCLOSURE', 'title', 'TITLE', \n",
    "                      'study', 'STUDY', 'case', 'CASE', 'analysis', 'ANALYSIS']\n",
    "    \n",
    "    stopwords.extend(avoiding_words)\n",
    "    \n",
    "    regex = r\"\\b[^\\d\\W]+\\b\"\n",
    "    tokens = []\n",
    "    sentences = nltk.sent_tokenize(cleantext)\n",
    "    for s in sentences:\n",
    "        words = re.findall(regex, s)\n",
    "        pairs = nltk.pos_tag(words)\n",
    "        for pair in pairs:\n",
    "            w = list(pair)[0] \n",
    "            tag = list(pair)[1]\n",
    "            if w.isupper() != True:w = w.lower() \n",
    "            if tag in pos_family['noun']:\n",
    "                w = lemmatizer.lemmatize(w, 'n')\n",
    "            elif tag in pos_family['pron']: # e.g their, self, what\n",
    "                w = lemmatizer.lemmatize(w)\n",
    "            elif tag in pos_family['verb']: # e.g experienced, based, evaluating, trying, healthcare\n",
    "                w = lemmatizer.lemmatize(w, 'v')\n",
    "            elif tag in pos_family['adj']: # e.g significant, pandemic, clinical, sensitive\n",
    "                w = lemmatizer.lemmatize(w, 'a')\n",
    "            elif tag in pos_family['adv']: #e.g. sore, seriously, alone, nationally\n",
    "                w = lemmatizer.lemmatize(w, 'r')\n",
    "            if w in stopwords: continue       \n",
    "            if w.isdigit(): continue\n",
    "            if len(w) <= 3: continue\n",
    "            tokens.append(w)\n",
    "            \n",
    "    cleaned_text = ' '.join(tokens)\n",
    " \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_process_large_json(filepath):\n",
    "    column_list = ['_id',\n",
    "    'abstract',\n",
    "    'URL',\n",
    "    'created',\n",
    "    'ISSN',\n",
    "    'container-title',\n",
    "    'author',\n",
    "    'DOI',\n",
    "    'published',\n",
    "    'subject',\n",
    "    'title',\n",
    "    'link',\n",
    "    'source',\n",
    "    'type',\n",
    "    'publisher',\n",
    "    'volume',\n",
    "    'last-updated',\n",
    "    'issue',\n",
    "    'funder',\n",
    "    'pubmed-abstract']\n",
    "    data = []\n",
    "    i = 0\n",
    "    with open(filepath, 'r') as json_file:\n",
    "        parser = ijson.items(json_file, 'item')\n",
    "        for value in parser:\n",
    "            i += 1\n",
    "            if i % 20000 == 0:\n",
    "                print('processed', i)\n",
    "            item = {}\n",
    "            for key, val in value.items():\n",
    "                if key in column_list:\n",
    "                    item[key] = val\n",
    "\n",
    "            item['title'] = str(item['title'])\n",
    "            item['language'] = language_detection(item['title'])\n",
    "\n",
    "            if item['language'] == 'en':\n",
    "                item['text'] = cleaning_text(item['title'])\n",
    "                del item['language']\n",
    "                if len(item['text']) >= 3:\n",
    "                    data.append(item)\n",
    "                \n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = read_process_large_json(INPUT_PATH+json1)\n",
    "print(f'\\n=== The number of records in \"{json1}\" : {len(df1)}')\n",
    "df2 = read_process_large_json(INPUT_PATH+json2)\n",
    "print(f'\\n=== The number of records in \"{json2}\": {len(df2)}')\n",
    "df3 = read_process_large_json(INPUT_PATH+json3)\n",
    "print(f'\\n=== The number of records in \"{json3}\": {len(df3)}')\n",
    "\n",
    "\n",
    "frames = [df1, df2, df3]\n",
    "covid_df = pd.concat(frames)\n",
    "covid_df.drop_duplicates(subset=['_id'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data preprocessing results\n",
    "covid_df.to_csv(OUTPUT_PATH + \"merged_covid_articles.tsv\", sep='\\t', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid_venv",
   "language": "python",
   "name": "covid_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "335.99px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
