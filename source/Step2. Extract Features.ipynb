{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Word2vec Model with COVID-19 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "import pickle\n",
    "from os.path import exists\n",
    "import os\n",
    "import nltk\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyparameters for generating word2vec model\n",
    "\n",
    "- vector_size (int, optional) – Dimensionality of the word vectors.\n",
    "- window (int, optional) – Maximum distance between the current and predicted word within a sentence.\n",
    "- min_count (int, optional) – Ignores all words with total frequency lower than this.\n",
    "- sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "- negative (int, optional) – If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "\n",
    "\n",
    "# Set hyparameters for extracting features\n",
    "**Extracting features**\n",
    "- We use TFIDF vectorizer instead of Count vectorizer for extracting features. \n",
    "\n",
    "**Parameters**\n",
    "- We have chosen a value of Minimum DF equal to 20 to get rid of extremely rare words that appear in less than 20 documents, and a Maximum DF equal to 80%. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 100\n",
    "WINDOW_SIZE = 5\n",
    "MIN_COUNT = 20\n",
    "SG = 1\n",
    "NEGATIVE = 20\n",
    "MIN_DF =  20\n",
    "MAX_DF = 0.8\n",
    "NORM_FUNCTION = 'l1'\n",
    "WEIGHT = 'tfidf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = '../coronavirus_twenty_years_of_research/covid_word2vec/'\n",
    "INPUT_DIR = '../coronavirus_twenty_years_of_research/technical_validation/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input data\n",
    "input_df = pd.read_pickle(INPUT_DIR + \"merged_covid_articles.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(df):\n",
    "    token_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        token_list.append(row['text'].split(' '))\n",
    "\n",
    "    training_docs = np.asarray(token_list, dtype = object)\n",
    "\n",
    "    return training_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_corpus(input_df)\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "dictionary.save(SAVE_DIR+'covid_dict.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"covid_\" + str(VECTOR_SIZE) + \"d.txt\"\n",
    "model_filename = \"covid_\" + str(VECTOR_SIZE) + \"d.model\"\n",
    "\n",
    "model = Word2Vec(corpus, \n",
    "                 vector_size=VECTOR_SIZE, \n",
    "                 window=WINDOW_SIZE, \n",
    "                 min_count=MIN_COUNT, \n",
    "                 sg=SG, \n",
    "                 negative=NEGATIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "if not exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "model.wv.save_word2vec_format(SAVE_DIR + \"/\" + filename, binary=False)\n",
    "model.save(SAVE_DIR + \"/\" + model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_object(obj, output_fname):\n",
    "    f = open(output_fname, 'wb')\n",
    "    pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_vectorization(weight, input_dir, corpus, _min_df, _max_df, norm_function):\n",
    "    \n",
    "    V_fname = input_dir + \"_V_{}.pkl\".format(weight)\n",
    "    D_fname = input_dir + \"_D_{}.pkl\".format(weight)\n",
    "    \n",
    "    if exists(V_fname):\n",
    "        print(\"File {} already exist\".format(V_fname))\n",
    "        V = pickle.load( open(V_fname, \"rb\") )\n",
    "        D = pickle.load( open(D_fname, \"rb\") )\n",
    "    else:\n",
    "        if weight == 'tfidf':\n",
    "            print('TfidfVectorizer is proceed')\n",
    "            V = TfidfVectorizer(analyzer='word', min_df=_min_df, norm=norm_function, max_df=_max_df, encoding='utf-8') # Term Frequency times inverse document frequency.\n",
    "            D = V.fit_transform(corpus)\n",
    "        else:\n",
    "            print('CountVectorizer is proceed')\n",
    "            V = CountVectorizer(analyzer='word', min_df=_min_df, max_df=_max_df, encoding='utf-8') \n",
    "            D = V.fit_transform(corpus)\n",
    "\n",
    "        # write the vectorizer and data\n",
    "        write_object(V, V_fname)\n",
    "        write_object(D, D_fname)\n",
    "\n",
    "    return V, D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer is proceed\n",
      "Matrix shape: (557956, 10989)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus = list(input_df['text'])\n",
    "V, D = data_vectorization(WEIGHT, INPUT_DIR, corpus, MIN_DF, MAX_DF, NORM_FUNCTION)\n",
    "print(\"Matrix shape:\", D.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid_venv",
   "language": "python",
   "name": "covid_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
