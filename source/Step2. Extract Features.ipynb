{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2. Extract Features\n",
    "\n",
    "This Jupyter notebook demonstrates how to convert the processed data into TF-IDF matrix and train a word2vec model with the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "import pickle\n",
    "from os.path import exists\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 100 \n",
    "WINDOW_SIZE = 5\n",
    "MIN_COUNT = 20\n",
    "SG = 1\n",
    "NEGATIVE = 20\n",
    "MIN_DF =  20\n",
    "MAX_DF = 0.8\n",
    "NORM_FUNCTION = 'l1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = '../data/processed_data/'\n",
    "INPUT_FILE = 'processed_data.tsv'\n",
    "OUTPUT_PATH = '../data/extracted_features/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read prossed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input data\n",
    "input_df = pd.read_csv(INPUT_PATH + INPUT_FILE, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert article data to TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_object(obj, output_fname):\n",
    "    f = open(output_fname, 'wb')\n",
    "    pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_vectorization(PATH, training_docs, MIN_DF, MAX_DF, NORM_FUNCTION):\n",
    "    \n",
    "    V_fname = PATH + \"covid_tfidf_v.pkl\"\n",
    "    D_fname = PATH + \"covid_tfidf_d.pkl\"\n",
    "    \n",
    "    if exists(V_fname):\n",
    "        print(\"File {} already exist\".format(V_fname))\n",
    "        V = pickle.load( open(V_fname, \"rb\") )\n",
    "        D = pickle.load( open(D_fname, \"rb\") )\n",
    "    else:\n",
    "        print('TfidfVectorizer is proceed')\n",
    "        V = TfidfVectorizer(analyzer='word', \n",
    "                            min_df=MIN_DF, \n",
    "                            max_df=MAX_DF, \n",
    "                            norm=NORM_FUNCTION, \n",
    "                            encoding='utf-8') # Term Frequency times inverse document frequency.\n",
    "        \n",
    "        D = V.fit_transform(training_docs)\n",
    "        \n",
    "        write_object(V, V_fname)\n",
    "        write_object(D, D_fname)\n",
    "\n",
    "    return V, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer is proceed\n",
      "Matrix shape: (557956, 10989)\n"
     ]
    }
   ],
   "source": [
    "training_docs = list(input_df['text'])\n",
    "V, D = data_vectorization(OUTPUT_PATH, training_docs, MIN_DF, MAX_DF, NORM_FUNCTION)\n",
    "print(\"Matrix shape:\", D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(df):\n",
    "    token_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        token_list.append(row['text'].split(' '))\n",
    "    training_docs = np.asarray(token_list)\n",
    "\n",
    "    return training_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_corpus(input_df)\n",
    "\n",
    "text_filename = \"covid_\" + str(VECTOR_SIZE) + \"d.txt\"\n",
    "model_filename = \"covid_\" + str(VECTOR_SIZE) + \"d.model\"\n",
    "\n",
    "model = Word2Vec(corpus, \n",
    "                 size=VECTOR_SIZE, \n",
    "                 window=WINDOW_SIZE, \n",
    "                 min_count=MIN_COUNT, \n",
    "                 sg=SG, \n",
    "                 negative=NEGATIVE)\n",
    "\n",
    "# save the trained model\n",
    "model.wv.save_word2vec_format(OUTPUT_PATH + text_filename, binary=False)\n",
    "model.save(OUTPUT_PATH + model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid_article_venv",
   "language": "python",
   "name": "covid_article_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
